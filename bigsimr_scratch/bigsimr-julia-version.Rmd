---
title: "Using bigsimr - Julia Version"
author: "Alex Knudson"
date: "2/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
```

# Setup

The following needs to be done at the start of every session.

```{r}
library(bigsimr)
Sys.setenv(JULIA_NUM_THREADS = parallel::detectCores())

# Either let JuliaCall find your installation of Julia or set manually
bs   <- bigsimr_setup(JULIA_HOME = "/home/alex/julia-1.5.3/bin/")
dist <- distributions_setup() # Julia distributions

JuliaCall::julia_eval("Threads.nthreads()")
```

```{r}
d <- 100
n <- 10000
system.time(bs$rmvn(n, bs$cor_randPD(d)))
```


# Getting Started

We’re going to show the basic use and syntax of *bigsimr* by using the New York air quality data set (airquality) included in the *RDatasets* package. We will focus specifically on the temperature (degrees Fahrenheit) and ozone level (parts per billion).

```{r}
df <- airquality %>%
  as_tibble() %>%
  select(Ozone, Temp) %>%
  drop_na()
df
```

Let’s look at the joint distribution of the Ozone and Temperature.

```{r, echo=FALSE}
p0 <- ggplot(df, aes(Temp, Ozone)) +
  geom_point(size = 1) +
  theme(legend.position = "none") +
  labs(x = "Temperature")

pTemp <- ggplot(df, aes(Temp)) + 
  geom_density() +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())

pOzone <- ggplot(df, aes(Ozone)) + 
  geom_density() + 
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  coord_flip() 

pTemp + plot_spacer() + p0 + pOzone + 
  plot_layout(widths = c(3,1), heights = c(1, 3))
```

We can see that not all margins are normally distributed; the ozone level is highly skewed. Though we don’t know the true distribution of ozone levels, we can go forward assuming that it is log-normally distributed.

To simulate observations from this joint distribution, we need to estimate the correlation and the marginal parameters.

## Estimating Correlation

To estimate the correlation, we use `cor` with an argument specifying the type of correlation to estimate. The options are `Pearson`, `Spearman`, or `Kendall`.

```{r}
p <- bs$cor(as.matrix(df), bs$Pearson)
p
```

## Defining Marginal Distributions

Next we can estimate the marginal parameters. Assuming that the `Temperature` is normally distributed, it has parameters:

```{r}
mu_temp <- mean(df$Temp)
sd_temp <- sd(df$Temp)
```

and assuming that `Ozone` is log-normally distributed, it has parameters:

```{r}
mu_ozone <- mean(log(df$Ozone))
sd_ozone <- sqrt(mean((log(df$Ozone) - mean(log(df$Ozone)))^2))
```

Finally we take the parameters and put them into a vector of margins:

```{r}
margins <- c(
  dist$Normal(mu_temp, sd_temp),
  dist$LogNormal(mu_ozone, sd_ozone)
)
margins
```

## Multivariate Distribution

While the individual components can be used separately within the package, they work best when joined together into a `MvDistribution` data type:

```{r}
D <- bs$MvDistribution(p, margins, bs$Pearson)
D
```

## Correlation Bounds

Given a vector of margins, the theoretical lower and upper correlation coefficients can be estimated using simulation:

```{r}
bounds <- bs$cor_bounds(D)
bounds$lower
bounds$upper
```

The `pearson_bounds` function uses more sophisticated methods to determine the theoretical lower and upper Pearson correlation bounds. It also requires more computational time.

```{r}
bounds <- bs$pearson_bounds(D)
bounds$lower
bounds$upper
```

## Simulating Multivariate Data

Let’s now simulate 10,000 observations from the joint distribution using `rvec`:

```{r}
x <- bs$rvec(10000, p, margins)
head(x)
```

Alternatively we can use `rand` with the `MvDistribution` type:

```{r}
bs$rand(D, 10)
```

# Pearson Matching

## Correlation Conversion

Let's say we really wanted to estimate the Spearman correlation between the temperature and ozone.

```{r}
ps <- bs$cor(as.matrix(df), bs$Spearman)
ps
```

```{r}
bs$cor_bounds(margins[1], margins[2], bs$Spearman)
```

If we just use the Spearman correlation when we simulate data, then the errors are double.

1. The NORTA algorithm is expecting a Pearson correlation
2. The non-linear transformation in the NORTA step does not guarantee that the input correlation is the same as the output.

```{r}
D2 <- bs$MvDistribution(ps, margins, bs$Spearman)
x2 <- bs$rand(D2, 1e6)
bs$cor(x2, bs$Spearman)
```

Let's try to address **(1)** and convert the Spearman correlation to a Pearson correlation.

```{r}
pp <- bs$cor_convert(ps, bs$Spearman, bs$Pearson)
D3 <- bs$MvDistribution(pp, margins, bs$Pearson)
x3 <- bs$rand(D3, 1e6)
bs$cor(x3, bs$Pearson)
bs$cor(x3, bs$Spearman)
```

Notice that the estimated Pearson correlation is lower than the target Pearson correlation, but the estimated Spearman correlation is essentially the same as the target. This is because the transformation in the NORTA step is monotonic, which means that rank-based correlations are preserved. As a consequence, we can match the Spearman correlation exactly (up to stochastic error), but not the Pearson.

## Pearson Matching

We can overcome this limitation by employing a Pearson matching algorithm that determines the necessary input correlation in order to achieve the target correlation. Let's now address **(2)**.

```{r}
D4 <- bs$pearson_match(D2)
bs$cor(D4)
```

Notice the significant change in the input correlation!

```{r}
x4 <- bs$rand(D4, 1e6)
bs$cor(x4, bs$Pearson)
```

But the estimated correlation is nearly spot on to the [converted] Pearson correlation (ρ_p).

A better example is using the `MvDistribution`. We never estimated the correlation after simulating, so let's look at that now.


```{r}
bs$cor(bs$rand(D, 1e6))
```

compared to the target correlation:

```{r}
p
```

The estimated correlation is much too low. Let's do some Pearson matching and observe the results.

```{r}
D5 <- bs$pearson_match(D)
x5 <- bs$rand(D5, 1e6)
bs$cor(x5)
```

Now the simulated data results in a correlation structure that exactly matches the target Pearson correlation!

# Nearest Correlation Matrix


Sometimes what we want really is the Spearman correlation. Then we don't need to do any Pearson matching. All we need to do is estimate/obtain the Spearman correlation of some data, convert it to Pearson, and then simulate. The resulting simulated data will have the same Spearman correlation as the one estimated from the data (up to stochastic error). The problem is that for high dimensional data, the Spearman or converted Pearson correlation matrix may not be positive semidefinite (PSD). The problem is then how to compute the nearest PSD correlation matrix.

We provide the function `cor_nearPD` to handle this problem. It is based off of the work of Qi and Sun (2006), and is a quadratically convergent algorithm. Here we use BRCA data to show its use.

```{r}
m <- as.matrix(brca)
str(m)
```

```{r}
fit_mom <- function(x) {
  m <- mean(x)
  s <- sd(x)
  r <- m^2 / (s^2 - m)
  p <- m / s^2
  dist$NegativeBinomial(r, p)
}
margins <- apply(m, 2, fit_mom)
margins <- do.call(c, unname(margins))
```

```{r}
tau <- bs$cor(m, bs$Spearman)
rho <- bs$cor_convert(tau, bs$Spearman, bs$Pearson)
matrixcalc::is.positive.definite(rho)
```

We see that the converted Pearson correlation matrix is no longer positive definite. This will result in a failure during the multivariate normal generation, particularly during the Cholesky decomposition.

```{r, error=TRUE}
# Fails because "rho" is not positive definite
bs$rvec(10, rho, margins)
```

We can fix this by computing the nearest PD correlation.

```{r}
rho_tilde <- bs$cor_nearPD(rho)
matrixcalc::is.positive.definite(rho_tilde)
```

```{r}
x <- bs$rvec(10, rho_tilde, margins)
str(x)
```

