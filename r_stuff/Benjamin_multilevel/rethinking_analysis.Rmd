---
title: "Ben Multilevel"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rethinking)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

dat <- readRDS("By_Year_State_updated.rds")
dat2 <- dat %>%
  select(Y_st, X_t, t) %>%
  drop_na()
```

# The model

- states
- 14 points per state
- $t$ - time
- $Y_st$ - dependent variable
- $X_t$ - independent variable

Linear Model

$$
\begin{align*}
Y &\sim \mathcal{N}(\mu, \sigma^2) \\
\mu &= \alpha + \beta X + \gamma t\\
\alpha &\sim \mathcal{N}(0, 100) \\
\beta &\sim \mathcal{N}(0, 100) \\
\gamma &\sim \mathcal{N}(0, 100) \\
\sigma &\sim \text{HalfCauchy}(0, 2.5)
\end{align*}
$$


```{r}
model <- alist(
  Y_st ~ normal(mu, sigma),
  mu <- a_s + b_s * X_t + g_s * t,
  a_s ~ normal(0, 100),
  b_s ~ normal(0, 100),
  g_s ~ normal(0, 100),
  
  sigma ~ half_cauchy(0, 2.5)
)

fit <- ulam(model, dat2)
```

```{r}
precis(fit, digits = 4)
```

```{r}
lm(Y_st ~ 1 + X_t + t, data = dat2)
```

# Multilevel

```{r}
dat3 <- dat %>%
  arrange(States) %>%
  mutate(State = factor(States)) %>%
  select(Y_st, X_t, t, State) %>%
  drop_na()

dat3
levels(dat3$State)
```

$$
\begin{align*}
Y_{st} &\sim \mathcal{N}(\mu_{st}, \sigma_{st}^2) \\
\mu_{st} &= \alpha_s + \beta_s X_t + \gamma_s t\\
\alpha_s &\sim \mathcal{N}(0, 100) \\
\beta_s &\sim \mathcal{N}(0, 100) \\
\gamma_s &\sim \mathcal{N}(0, 100) \\
\sigma &\sim \text{HalfCauchy}(0, 2.5)
\end{align*}
$$

```{r}
m2 <- alist(
  Y_st ~ normal(mu, sigma[State]),
  mu <- a_s[State] + b_s[State] * X_t + g_s[State] * t,
  a_s[State] ~ normal(0, 100),
  b_s[State] ~ normal(0, 100),
  g_s[State] ~ normal(0, 100),
  
  sigma[State] ~ half_cauchy(0, 2.5)
)

fit2 <- ulam(m2, dat3, chains = 8, iter = 10000)
precis(fit2, digits = 4, depth = 2)
post2 <- extract.samples(fit2)
```

```{r}
m3 <- alist(
  Y_st ~ normal(mu, sigma[State]),
  mu <- (a + a_s[State]) + (b + b_s[State]) * X_t + (g + g_s[State]) * t,
  c(a, b, g) ~ normal(0, 100),
  a_s[State] ~ normal(0, 100),
  b_s[State] ~ normal(0, 100),
  g_s[State] ~ normal(0, 100),
  
  sigma[State] ~ half_cauchy(0, 2.5)
)

fit3 <- ulam(m3, dat3, chains = 4, cores = 4, iter = 4000, warmup = 2000,
             control = list(max_treedepth = 12, adapt_delta = 0.99))
precis(fit3, digits = 4)
```

```{r}
m4 <- alist(
  Y_st ~ normal(mu, sigma[State]),
  mu <- (a + a_s[State]) + (b + b_s[State]) * X_t + (g + g_s[State]) * t,
  c(a, b, g) ~ normal(0, 100),
  a_s[State] ~ normal(0, sd_a),
  b_s[State] ~ normal(0, sd_b),
  g_s[State] ~ normal(0, sd_g),
  
  sigma[State] ~ half_cauchy(0, 2.5),
  c(sd_a, sd_b, sd_g) ~ half_cauchy(0, 2.5)
)

fit4 <- ulam(m4, dat3, chains = 4, cores = 4, iter = 4000, warmup = 2000,
             control = list(max_treedepth = 12, adapt_delta = 0.99))
```


```{r}
library(rstanarm)
Model2<- stan_lmer(Y_st ~ 1 + X_t + t + (1 + X_t + t | State),
```

$$
\begin{align*}
Y_{st} &\sim \mathcal{N}(\mu, \sigma) \\
\mu &= \alpha + \alpha_s + (\beta + \beta_s) X_t + (\gamma + \gamma_s) t \\
\alpha &\sim \mathcal{N}(0, 5) \\
\alpha_s &\sim \mathcal{N}(0, 5) \\
\beta, \gamma &\sim \mathcal{N}(0, 10) \\
\beta_s, \gamma_s &\sim \mathcal{N}(0, 10)
\end{align*}
$$

```{r, eval=FALSE}
Model2<- stan_lmer(formula = Y_st ~ 1 + X_t + t + (1 + X_t + t | States), 
          data = test, cores = CORES, iter = 1e4, control = list(adapt_delta = 0.99),
          prior = normal(location = 0,
                         scale = 5,
                         autoscale = FALSE),
          prior_intercept = normal(location = 0, 
                                   scale = 10, 
                                   autoscale = FALSE),
          seed = SEED)
```


```{r}
data = dat3, cores = 4, iter = 1e4, seed = 2000, chains = 4,
          control = list(adapt_delta = 0.99, max_treedepth = 12),
          prior = normal(location = 0,
                         scale = 5,
                         autoscale = FALSE),
          prior_intercept = normal(location = 0, 
                                   scale = 5, 
                                   autoscale = FALSE))
```

```{r}
post <- as.data.frame(as.matrix(Model2))
mu_a_sims <- as.matrix(Model2, pars = "(Intercept)") # draws for 73 schools' school-level error 
u_sims <- as.matrix(Model2, regex_pars = "b\\[\\(Intercept\\) State\\:")
a_sims <- mu_a_sims %*% matrix(1, 1, 50) + u_sims
p90 <- apply(a_sims, 2, quantile, probs = c(0.05, 0.5, 0.95))
```


```{r}
post2 <- extract.samples(fit4)
a_sims2 <- replicate(50, post2$a) + post2$a_s
p90_2 <- apply(a_sims2, 2, quantile, probs = c(0.05, 0.5, 0.95))
colnames(p90_2) <- colnames(p90)
```


```{r}
dat_train <- dat3 %>%
  filter(t != -1)

dat_test <- dat3 %>%
  filter(t == -1)

fit_train <- ulam(m4, dat_train, chains = 4, cores = 4, iter = 4000, warmup = 2000,
             control = list(max_treedepth = 12, adapt_delta = 0.99))

predict_test <- link(fit_train, data = dat_test)

pred_dat <- predict_test %>%
  as_tibble(rownames = NULL)

colnames(pred_dat) <- levels(dat3$State)

pred_dat2 <- pred_dat %>%
  pivot_longer(1:50, names_to = "State", values_to = "y_hat") %>%
  group_by(State) %>%
  summarise(y_mu  = mean(y_hat),
            y_med = median(y_hat),
            l90   = quantile(y_hat, 0.05),
            u90   = quantile(y_hat, 0.95)) %>%
  add_column(y = dat_test$Y_st, .after = 1) %>%
  mutate(State = factor(State))
```

```{r, fig.width = 8, fig.height=8}
pred_dat2 %>%
  ggplot(aes(fct_reorder(State, y_med, sort), y = y)) +
  geom_point(aes(y = y_med)) +
  geom_errorbar(aes(ymin = l90, ymax = u90)) + 
  geom_point(color = "blue") + 
  coord_flip()
```

